{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPQkjf1Ea4P103v8DbDcc9m"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nw9GQLdyhJi7"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax(x):\n",
        "  if x.ndim == 2:\n",
        "      x = x.T\n",
        "      x = x - np.max(x, axis=0)\n",
        "      y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "      return y.T \n",
        "\n",
        "  x = x - np.max(x) # 오버플로 대책\n",
        "  return np.exp(x) / np.sum(np.exp(x))\n",
        "\n",
        "def cross_entropy_error(y, t):\n",
        "  if y.ndim == 1:\n",
        "      t = t.reshape(1, t.size)\n",
        "      y = y.reshape(1, y.size)\n",
        "      \n",
        "  # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
        "  if t.size == y.size:\n",
        "      t = t.argmax(axis=1)\n",
        "            \n",
        "  batch_size = y.shape[0]\n",
        "  return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
        "\n",
        "def numerical_gradient(f, x):\n",
        "  h = 1e-4 # 0.0001\n",
        "  grad = np.zeros_like(x)\n",
        "  \n",
        "  it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
        "  while not it.finished:\n",
        "      idx = it.multi_index\n",
        "      tmp_val = x[idx]\n",
        "      x[idx] = float(tmp_val) + h\n",
        "      fxh1 = f(x) # f(x+h)\n",
        "      \n",
        "      x[idx] = tmp_val - h \n",
        "      fxh2 = f(x) # f(x-h)\n",
        "      grad[idx] = (fxh1 - fxh2) / (2*h)\n",
        "      \n",
        "      x[idx] = tmp_val # 값 복원\n",
        "      it.iternext()   \n",
        "      \n",
        "  return grad\n",
        "\n",
        "class simpleNet:\n",
        "  def __init__(self):\n",
        "    self.W = np.random.randn(2,3)\n",
        "\n",
        "  def predict(self, x):\n",
        "    return np.dot(x, self.W)\n",
        "  \n",
        "  def loss(self, x, t):\n",
        "    z = self.predict(x)\n",
        "    y = softmax(z)\n",
        "    loss = cross_entropy_error(y, t)\n",
        "\n",
        "    return loss"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcXKTtD0onQY",
        "outputId": "fdb7f67d-61b5-4876-8f95-9fd35d654f86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "net = simpleNet()\n",
        "print(net.W)\n",
        "\n",
        "x = np.array([0.6, 0.9])\n",
        "p = net.predict(x)\n",
        "print(p)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-1.18354504 -0.03376404  1.16546915]\n",
            " [-0.44856451 -0.5845412  -0.68368063]]\n",
            "[-1.11383508 -0.5463455   0.08396892]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uNncS5FpLNq",
        "outputId": "d9a38d9f-b976-46d0-a3a9-8a23e79b4a72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "np.argmax(p)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4-lgvzDpYPH",
        "outputId": "f85c1c88-4ebd-4387-cb81-2982a8adbf10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "t = np.array([0,0,1])\n",
        "net.loss(x,t)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6066522472899063"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_DfAQv-py5f",
        "outputId": "3cafa1b6-a252-4578-f237-6f042255519a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#def f(W):\n",
        "#  return net.loss(x,t)\n",
        "f = lambda w: net.loss(x,t)\n",
        "\n",
        "dW = numerical_gradient(f, net.W)\n",
        "print(dW)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.09873831  0.17415795 -0.27289626]\n",
            " [ 0.14810747  0.26123692 -0.40934439]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tmEw9gMp6GS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}